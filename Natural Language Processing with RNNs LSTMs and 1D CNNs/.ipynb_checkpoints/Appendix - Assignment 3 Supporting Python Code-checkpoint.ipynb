{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d1766d",
   "metadata": {},
   "source": [
    "# Appendix _______________________ - Supporting Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe37b46",
   "metadata": {},
   "source": [
    "#### Steve Desilets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8de223",
   "metadata": {},
   "source": [
    "#### November 5, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4c312",
   "metadata": {},
   "source": [
    "## 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab339f5a",
   "metadata": {},
   "source": [
    "In this notebook, we aim to build natural language processing pipelines capable of effectively classifying text articles into their respective article categories. The underlying data that we leverage is the AG_News dataset, which includes over one million news articles corresponding to four categories. We aim to build a variety of models, including artificial neural networks, recurrent neural networks, long short term memory models, and transformer-based models to discover which methods are most effective for classifying articles into their respective categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37146c59",
   "metadata": {},
   "source": [
    "### 1.1) Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4423b5",
   "metadata": {},
   "source": [
    "First, let's set up this notebook by importing the relevant packages and by defining functions that we will use throughout our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e13f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from packaging import version\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70812086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc1c40",
   "metadata": {},
   "source": [
    "We can verify the version of TensorFlow in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c668e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook requires TensorFlow 2.0 or above\n",
      "TensorFlow version:  2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251409d4",
   "metadata": {},
   "source": [
    "Let's define visualization functions that we'll use throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bca5b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_report(test_labels, predictions):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(test_labels, predictions))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(test_labels, predictions)))\n",
    "    print('Root Mean Square Error: {}'.format(np.sqrt(MSE(test_labels, predictions))))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "  ax = plt.subplot(subplot)\n",
    "  ax.plot(training)\n",
    "  ax.plot(validation)\n",
    "  ax.set_title('model '+ title)\n",
    "  ax.set_ylabel(title)\n",
    "  ax.set_xlabel('epoch')\n",
    "  ax.legend(['training', 'validation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec806efd",
   "metadata": {},
   "source": [
    "Let's mount to the Google Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aeed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3091fb3",
   "metadata": {},
   "source": [
    "### 1.2) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd1a3f",
   "metadata": {},
   "source": [
    "Now that we've set up our notebook, let's load the subset of the AG news dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8648f997",
   "metadata": {},
   "source": [
    "### Load AG News Subset Data\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b> ag_news_subset</b><br>\n",
    "    See https://www.tensorflow.org/datasets/catalog/ag_news_subset\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321028b7",
   "metadata": {},
   "source": [
    "Get all the words in the documents (as well as the number of words in each document) by using the encoder to get the indices associated with each token and then translating the indices to tokens. But first we need to get the \"unpadded\" new articles so that we can get their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354fe3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
    "!python -m tensorflow_datasets.scripts.download_and_prepare \\\n",
    "        --register_checksums --datasets=ag_news_subset\n",
    "\n",
    "# https://www.tensorflow.org/datasets/splits\n",
    "# The full `train` and `test` splits, interleaved together.\n",
    "ri = tfds.core.ReadInstruction('train') + tfds.core.ReadInstruction('test')\n",
    "dataset_all, info = tfds.load('ag_news_subset', with_info=True,  split=ri, as_supervised=True)\n",
    "text_only_dataset_all=dataset_all.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3fb713",
   "metadata": {},
   "source": [
    "Let's conduct exploratory data analysis of this ag_news_subset dataset. We combined the training and test data for a total of 127,600 news articles. We can begin by observing the first 10 rows of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b71ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.as_dataframe(dataset_all.take(10),info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c00ff",
   "metadata": {},
   "source": [
    "Let's review the labels for the categories of articles in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories =dict(enumerate(info.features[\"label\"].names))\n",
    "print(f'Dictionary: ',categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed216c99",
   "metadata": {},
   "source": [
    "Let's observe the number of observations that correspond to each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d09e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categories = [categories[label] for label in dataset_all.map(lambda text, label: label).as_numpy_iterator()]\n",
    "Counter(train_categories).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8952a62",
   "metadata": {},
   "source": [
    "We see that the 127,600 articles are evenly distributed across the four classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d2e7c",
   "metadata": {},
   "source": [
    "Let's do a bit of data preprocessing to enable further exploratory data analysis. \n",
    "\n",
    "We can start by making the corpus all lowercase, stripping punctuation, and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517541fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stopwords(input_text):\n",
    "    lowercase = tf.strings.lower(input_text)\n",
    "    stripped_punct = tf.strings.regex_replace(lowercase\n",
    "                                  ,'[%s]' % re.escape(string.punctuation)\n",
    "                                  ,'')\n",
    "    return tf.strings.regex_replace(stripped_punct, r'\\b(' + r'|'.join(STOPWORDS) + r')\\b\\s*',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae1a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords',quiet=True)\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244cbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_tokens = None\n",
    "text_vectorization=layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_stopwords\n",
    ")\n",
    "text_vectorization.adapt(text_only_dataset_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "doc_sizes = []\n",
    "corpus = []\n",
    "for example, _ in dataset_all.as_numpy_iterator():\n",
    "  enc_example = text_vectorization(example)\n",
    "  doc_sizes.append(len(enc_example))\n",
    "  corpus+=list(enc_example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(corpus)} words in the corpus of {len(doc_sizes)} news articles.\")\n",
    "print(f\"Each news article has between {min(doc_sizes)} and {max(doc_sizes)} tokens in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(text_vectorization.get_vocabulary())} vocabulary words in the corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ccb3d",
   "metadata": {},
   "source": [
    "Let's observe the first 50 words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef45f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(text_vectorization.get_vocabulary())\n",
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1209",
   "metadata": {},
   "source": [
    "Let's examine the distribution of the number of tokens per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da883b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.hist(doc_sizes, bins=20,range = (0,80))\n",
    "plt.xlabel(\"Tokens Per Document\")\n",
    "plt.ylabel(\"Number of AG News Articles\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee5e4f3",
   "metadata": {},
   "source": [
    "### 1.3) Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f3342",
   "metadata": {},
   "source": [
    "Now that we've conducted exploratory data analysis, let's preprocess the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb736e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
    "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=ag_news_subset\n",
    "\n",
    "dataset,info=\\\n",
    "tfds.load('ag_news_subset', with_info=True,  split=['train[:95%]','train[95%:]', 'test'],batch_size = 32\n",
    "          , as_supervised=True)\n",
    "\n",
    "train_ds, val_ds, test_ds = dataset\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34922a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 96\n",
    "max_tokens = 1000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    "    standardize=custom_stopwords\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be201b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64822f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0cfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05852f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d4fa586",
   "metadata": {},
   "source": [
    "## 2) Model 1 - Bi-Directional Long Short Term Memory (LSTM) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153925b7",
   "metadata": {},
   "source": [
    "### 2.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b54a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens\n",
    "                            ,output_dim=256\n",
    "                            ,mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"Model_One\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "history=model.fit(int_train_ds, validation_data=int_val_ds, epochs=200, callbacks=callbacks)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "runtime = end_time - start_time\n",
    "print(f\"The runtime to fit this model was: {runtime}.\")\n",
    "\n",
    "\n",
    "model = keras.models.load_model(\"Model_One\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5263e5",
   "metadata": {},
   "source": [
    "### 2.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b89eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8591eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85fffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc55fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1d069",
   "metadata": {},
   "source": [
    "## 3) Model 2 - 1-Dimensional Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39811306",
   "metadata": {},
   "source": [
    "### 3.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38f369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c41fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3013a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e22c24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402df02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Conv1D(filters=32, kernel_size=3, activation='relu')(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"Model_Two\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "history=model.fit(int_train_ds, validation_data=int_val_ds, epochs=200, callbacks=callbacks)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "runtime = end_time - start_time\n",
    "print(f\"The runtime to fit this model was: {runtime}.\")\n",
    "\n",
    "model = keras.models.load_model(\"Model_Two\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef132164",
   "metadata": {},
   "source": [
    "### 3.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c732014",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f3eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05945924",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb83f20",
   "metadata": {},
   "source": [
    "## 4) Model 3 - _______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5f6fd",
   "metadata": {},
   "source": [
    "### 4.1) Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b37c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99862b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd61304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2eec383",
   "metadata": {},
   "source": [
    "### 4.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625965a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e56367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52020ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec443c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26aa64",
   "metadata": {},
   "source": [
    "## 5) Model 4 - _______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651b311",
   "metadata": {},
   "source": [
    "### 5.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6dd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112a6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ce7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d04754c5",
   "metadata": {},
   "source": [
    "### 5.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac98d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6570949",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e1a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba731df7",
   "metadata": {},
   "source": [
    "## 6) Model 5 - ____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6b080",
   "metadata": {},
   "source": [
    "### 6.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee6a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a20fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0f4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74615d65",
   "metadata": {},
   "source": [
    "### 6.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cffe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38bdf8",
   "metadata": {},
   "source": [
    "## 7) Model 6 - _____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919ac03",
   "metadata": {},
   "source": [
    "### 7.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d86890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d45f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05555cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d610a96",
   "metadata": {},
   "source": [
    "### 7.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e8c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c048e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d08353",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7035a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63569340",
   "metadata": {},
   "source": [
    "## 8) Model 7 - _________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c047cb",
   "metadata": {},
   "source": [
    "### 8.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87639fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787934fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac5f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28a15e0b",
   "metadata": {},
   "source": [
    "### 8.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590d035",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38969ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fd443",
   "metadata": {},
   "source": [
    "## 9) Model 8 - ___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480ca2f0",
   "metadata": {},
   "source": [
    "### 9.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634fdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91030918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa055a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1632ee",
   "metadata": {},
   "source": [
    "### 9.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b78346",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba009162",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffd8ff",
   "metadata": {},
   "source": [
    "## 10) Model 9 - ___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2b740",
   "metadata": {},
   "source": [
    "### 10.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf6864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc9d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f690523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e51a6d8b",
   "metadata": {},
   "source": [
    "### 10.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c073fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6969c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261a6df",
   "metadata": {},
   "source": [
    "## 11) Model 10 - _______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f529ec",
   "metadata": {},
   "source": [
    "### 11.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ddf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba5d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b226f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f596a50b",
   "metadata": {},
   "source": [
    "### 11.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf18cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04507c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8589647",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
