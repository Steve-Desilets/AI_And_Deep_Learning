{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d1766d",
   "metadata": {},
   "source": [
    "# Appendix _______________________ - Supporting Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe37b46",
   "metadata": {},
   "source": [
    "#### Steve Desilets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8de223",
   "metadata": {},
   "source": [
    "#### November 5, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4c312",
   "metadata": {},
   "source": [
    "## 1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab339f5a",
   "metadata": {},
   "source": [
    "In this notebook, we aim to build natural language processing pipelines capable of effectively classifying text articles into their respective article categories. The underlying data that we leverage is the AG_News dataset, which includes over one million news articles corresponding to four categories. We aim to build a variety of models, including artificial neural networks, recurrent neural networks, long short term memory models, and transformer-based models to discover which methods are most effective for classifying articles into their respective categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37146c59",
   "metadata": {},
   "source": [
    "### 1.1) Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4423b5",
   "metadata": {},
   "source": [
    "First, let's set up this notebook by importing the relevant packages and by defining functions that we will use throughout our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43e13f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from packaging import version\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70812086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc1c40",
   "metadata": {},
   "source": [
    "We can verify the version of TensorFlow in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c668e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook requires TensorFlow 2.0 or above\n",
      "TensorFlow version:  2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(\"This notebook requires TensorFlow 2.0 or above\")\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251409d4",
   "metadata": {},
   "source": [
    "Let's define visualization functions that we'll use throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bca5b650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_report(test_labels, predictions):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(test_labels, predictions))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(test_labels, predictions)))\n",
    "    print('Root Mean Square Error: {}'.format(np.sqrt(MSE(test_labels, predictions))))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "\n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "  ax = plt.subplot(subplot)\n",
    "  ax.plot(training)\n",
    "  ax.plot(validation)\n",
    "  ax.set_title('model '+ title)\n",
    "  ax.set_ylabel(title)\n",
    "  ax.set_xlabel('epoch')\n",
    "  ax.legend(['training', 'validation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec806efd",
   "metadata": {},
   "source": [
    "Let's mount to the Google Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aeed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3091fb3",
   "metadata": {},
   "source": [
    "### 1.2) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd1a3f",
   "metadata": {},
   "source": [
    "Now that we've set up our notebook, let's load the subset of the AG news dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8648f997",
   "metadata": {},
   "source": [
    "### Load AG News Subset Data\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b> ag_news_subset</b><br>\n",
    "    See https://www.tensorflow.org/datasets/catalog/ag_news_subset\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321028b7",
   "metadata": {},
   "source": [
    "Get all the words in the documents (as well as the number of words in each document) by using the encoder to get the indices associated with each token and then translating the indices to tokens. But first we need to get the \"unpadded\" new articles so that we can get their length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "354fe3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO[build.py]: Loading dataset ag_news_subset from imports: tensorflow_datasets.datasets.ag_news_subset.ag_news_subset_dataset_builder\n",
      "INFO[build.py]: download_and_prepare for dataset ag_news_subset/1.0.0...\n",
      "INFO[dataset_builder.py]: Generating dataset ag_news_subset (C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0)\n",
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0...\n",
      "INFO[download_manager.py]: Downloading https://drive.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms into C:\\Users\\steve\\tensorflow_datasets\\downloads\\ucexport_download_id_0Bz8a_Dbh9QhbUDNpeUdjb0wxj4g1umFAV8OV-uDwxSJR0LdxO_k1jxMuFWwAfNX9jos.tmp.280cf5bf229143859cfd266740b73e15...\n",
      "INFO[writer.py]: Done writing C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*. Number of examples: 120000 (shards: [120000])\n",
      "INFO[writer.py]: Done writing C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-test.tfrecord*. Number of examples: 7600 (shards: [7600])\n",
      "Dataset ag_news_subset downloaded and prepared to C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0. Subsequent calls will reuse this data.\n",
      "INFO[build.py]: Dataset generation complete...\n",
      "\n",
      "tfds.core.DatasetInfo(\n",
      "    name='ag_news_subset',\n",
      "    full_name='ag_news_subset/1.0.0',\n",
      "    description=\"\"\"\n",
      "    AG is a collection of more than 1 million news articles. News articles have been\n",
      "    gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of\n",
      "    activity. ComeToMyHead is an academic news search engine which has been running\n",
      "    since July, 2004. The dataset is provided by the academic comunity for research\n",
      "    purposes in data mining (clustering, classification, etc), information retrieval\n",
      "    (ranking, search, etc), xml, data compression, data streaming, and any other\n",
      "    non-commercial activity. For more information, please refer to the link\n",
      "    http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
      "    \n",
      "    The AG's news topic classification dataset is constructed by Xiang Zhang\n",
      "    (xiang.zhang@nyu.edu) from the dataset above. It is used as a text\n",
      "    classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann\n",
      "    LeCun. Character-level Convolutional Networks for Text Classification. Advances\n",
      "    in Neural Information Processing Systems 28 (NIPS 2015).\n",
      "    \n",
      "    The AG's news topic classification dataset is constructed by choosing 4 largest\n",
      "    classes from the original corpus. Each class contains 30,000 training samples\n",
      "    and 1,900 testing samples. The total number of training samples is 120,000 and\n",
      "    testing 7,600.\n",
      "    \"\"\",\n",
      "    homepage='https://arxiv.org/abs/1509.01626',\n",
      "    data_path='C:\\\\Users\\\\steve\\\\tensorflow_datasets\\\\ag_news_subset\\\\1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=11.24 MiB,\n",
      "    dataset_size=35.79 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'description': Text(shape=(), dtype=string),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=4),\n",
      "        'title': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('description', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=7600, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=120000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@misc{zhang2015characterlevel,\n",
      "        title={Character-level Convolutional Networks for Text Classification},\n",
      "        author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n",
      "        year={2015},\n",
      "        eprint={1509.01626},\n",
      "        archivePrefix={arXiv},\n",
      "        primaryClass={cs.LG}\n",
      "    }\"\"\",\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1105 01:22:53.796984 24656 download_and_prepare.py:46] ***`tfds build` should be used instead of `download_and_prepare`.***\n",
      "\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "                                       \n",
      "\n",
      "\n",
      "\n",
      "                                                 \n",
      "\u001b[A\u001b[A\n",
      "\n",
      "                                  \n",
      "\u001b[A\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "\n",
      "Dl Size...:   0%|          | 0/11 [00:10<?, ? MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Size...:   9%|9         | 1/11 [00:10<01:47, 10.71s/ MiB]\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "\n",
      "Dl Size...:   9%|9         | 1/11 [00:10<01:47, 10.71s/ MiB]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "\n",
      "Dl Size...:  18%|#8        | 2/11 [00:10<01:36, 10.71s/ MiB]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Size...:  27%|##7       | 3/11 [00:10<00:22,  2.82s/ MiB]\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "\n",
      "Dl Size...:  27%|##7       | 3/11 [00:10<00:22,  2.82s/ MiB]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "\n",
      "Dl Size...:  36%|###6      | 4/11 [00:10<00:19,  2.82s/ MiB]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Size...:  45%|####5     | 5/11 [00:10<00:08,  1.41s/ MiB]\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "\n",
      "Dl Size...:  45%|####5     | 5/11 [00:10<00:08,  1.41s/ MiB]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\n",
      "\n",
      "Dl Size...:  55%|#####4    | 6/11 [00:10<00:07,  1.41s/ MiB]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:10, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "\n",
      "Dl Size...:  64%|######3   | 7/11 [00:11<00:05,  1.41s/ MiB]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Size...:  73%|#######2  | 8/11 [00:11<00:02,  1.44 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "\n",
      "Dl Size...:  73%|#######2  | 8/11 [00:11<00:02,  1.44 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "\n",
      "Dl Size...:  82%|########1 | 9/11 [00:11<00:01,  1.44 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Size...:  91%|######### | 10/11 [00:11<00:00,  2.06 MiB/s]\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "\n",
      "Dl Size...:  91%|######### | 10/11 [00:11<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:11<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|##########| 1/1 [00:11<00:00, 11.82s/ url]\n",
      "Dl Completed...: 100%|##########| 1/1 [00:11<00:00, 11.82s/ url]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:11<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 0 file [00:11, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|##########| 1/1 [00:12<00:00, 11.82s/ url]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:12<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...:   0%|          | 0/1 [00:12<?, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|##########| 1/1 [00:12<00:00, 11.82s/ url]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:12<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...:   0%|          | 0/2 [00:12<?, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|##########| 1/1 [00:12<00:00, 11.82s/ url]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:12<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...:   0%|          | 0/3 [00:12<?, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|##########| 1/1 [00:12<00:00, 11.82s/ url]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:12<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...:   0%|          | 0/4 [00:12<?, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...:  25%|##5       | 1/4 [00:12<00:37, 12.34s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|##########| 1/1 [00:12<00:00, 11.82s/ url]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:12<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...:  25%|##5       | 1/4 [00:12<00:37, 12.34s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|##########| 1/1 [00:12<00:00, 11.82s/ url]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:12<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...:  50%|#####     | 2/4 [00:12<00:24, 12.34s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|##########| 1/1 [00:12<00:00, 11.82s/ url]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:12<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...:  75%|#######5  | 3/4 [00:12<00:12, 12.34s/ file]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 100%|##########| 4/4 [00:12<00:00,  2.44s/ file]\u001b[A\u001b[A\n",
      "Dl Completed...: 100%|##########| 1/1 [00:12<00:00, 11.82s/ url]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:12<00:00,  2.06 MiB/s]\u001b[A\n",
      "\n",
      "\n",
      "Extraction completed...: 100%|##########| 4/4 [00:12<00:00,  2.44s/ file]\u001b[A\u001b[A\n",
      "Extraction completed...: 100%|##########| 4/4 [00:12<00:00,  3.19s/ file]\n",
      "\n",
      "Dl Size...: 100%|##########| 11/11 [00:12<00:00,  1.16s/ MiB]\n",
      "\n",
      "Dl Completed...: 100%|##########| 1/1 [00:12<00:00, 12.75s/ url]\n",
      "\n",
      "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]\n",
      "\n",
      "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 936 examples [00:00, 9309.16 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 1929 examples [00:00, 9667.66 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 2917 examples [00:00, 9756.34 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 3893 examples [00:00, 9637.31 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 4903 examples [00:00, 9757.59 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 5902 examples [00:00, 9824.55 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 6907 examples [00:00, 9884.25 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 7896 examples [00:00, 9874.40 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 8886 examples [00:00, 9858.48 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 9873 examples [00:01, 9845.52 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 10873 examples [00:01, 9870.39 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 11901 examples [00:01, 9990.83 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 12901 examples [00:01, 9983.78 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 13900 examples [00:01, 9845.36 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 14885 examples [00:01, 9686.45 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 15904 examples [00:01, 9824.31 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 16890 examples [00:01, 9832.70 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 17874 examples [00:01, 9834.17 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 18901 examples [00:01, 9945.21 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 19896 examples [00:02, 9868.92 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 20923 examples [00:02, 9972.65 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 21954 examples [00:02, 10059.40 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 22972 examples [00:02, 10078.42 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 23981 examples [00:02, 10027.29 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 24984 examples [00:02, 10008.17 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 26009 examples [00:02, 10062.21 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 27043 examples [00:02, 10130.47 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 28091 examples [00:02, 10209.44 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 29125 examples [00:02, 10229.87 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 30149 examples [00:03, 10189.66 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 31169 examples [00:03, 10087.00 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 32178 examples [00:03, 10030.88 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 33191 examples [00:03, 10056.79 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 34202 examples [00:03, 10046.42 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 35248 examples [00:03, 10149.99 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 36270 examples [00:03, 10159.36 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 37293 examples [00:03, 10164.24 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 38310 examples [00:03, 10023.35 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 39313 examples [00:03, 9925.17 examples/s] \u001b[A\n",
      "\n",
      "Generating train examples...: 40306 examples [00:04, 9889.54 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 41296 examples [00:04, 9808.71 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 42278 examples [00:04, 9693.50 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 43248 examples [00:04, 9633.76 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 44220 examples [00:04, 9643.77 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 45216 examples [00:04, 9719.61 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 46189 examples [00:04, 9559.13 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 47215 examples [00:04, 9740.31 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 48237 examples [00:04, 9864.65 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 49246 examples [00:04, 9918.90 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 50246 examples [00:05, 9917.33 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 51254 examples [00:05, 9962.02 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 52260 examples [00:05, 9965.27 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 53278 examples [00:05, 10002.86 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 54307 examples [00:05, 10059.46 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 55330 examples [00:05, 10102.65 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 56365 examples [00:05, 10169.36 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 57383 examples [00:05, 10140.21 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 58398 examples [00:05, 10102.06 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 59412 examples [00:05, 10110.73 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 60440 examples [00:06, 10156.13 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 61469 examples [00:06, 10185.80 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 62488 examples [00:06, 10017.29 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 63493 examples [00:06, 9999.09 examples/s] \u001b[A\n",
      "\n",
      "Generating train examples...: 64494 examples [00:06, 9910.72 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 65486 examples [00:06, 9883.63 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 66475 examples [00:06, 9884.52 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 67485 examples [00:06, 9945.21 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 68487 examples [00:06, 9966.82 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 69486 examples [00:06, 9956.55 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 70501 examples [00:07, 10012.07 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 71503 examples [00:07, 9988.90 examples/s] \u001b[A\n",
      "\n",
      "Generating train examples...: 72502 examples [00:07, 9826.63 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 73486 examples [00:07, 9720.34 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 74487 examples [00:07, 9779.07 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 75466 examples [00:07, 9767.16 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 76444 examples [00:07, 9622.58 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 77407 examples [00:07, 9482.79 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 78366 examples [00:07, 9505.68 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 79380 examples [00:08, 9673.22 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 80348 examples [00:08, 9640.41 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 81313 examples [00:08, 9511.29 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 82267 examples [00:08, 9519.11 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 83261 examples [00:08, 9639.66 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 84362 examples [00:08, 10019.80 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 85512 examples [00:08, 10452.88 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 86775 examples [00:08, 11091.70 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 88006 examples [00:08, 11451.76 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 89288 examples [00:08, 11828.22 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 90757 examples [00:09, 12672.97 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 92136 examples [00:09, 12969.74 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 93552 examples [00:09, 13292.47 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 95108 examples [00:09, 13959.83 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 96558 examples [00:09, 14107.09 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 98069 examples [00:09, 14400.66 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 99565 examples [00:09, 14561.12 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 101111 examples [00:09, 14815.00 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 102593 examples [00:09, 14769.05 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 104071 examples [00:09, 14336.80 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 105550 examples [00:10, 14430.49 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 107014 examples [00:10, 14490.28 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 108484 examples [00:10, 14551.42 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 109941 examples [00:10, 14358.40 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 111379 examples [00:10, 14298.84 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 112865 examples [00:10, 14444.92 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 114337 examples [00:10, 14498.62 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 115788 examples [00:10, 14330.60 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 117222 examples [00:10, 14042.37 examples/s]\u001b[A\n",
      "\n",
      "Generating train examples...: 118628 examples [00:10, 13868.65 examples/s]\u001b[A\n",
      "\n",
      "                                                                          \u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:   0%|          | 0/120000 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:   0%|          | 1/120000 [00:00<3:58:39,  8.38 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:   6%|6         | 7511/120000 [00:00<00:02, 40735.89 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  11%|#1        | 13299/120000 [00:00<00:02, 44329.04 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  17%|#7        | 20735/120000 [00:00<00:01, 55114.69 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  23%|##3       | 27944/120000 [00:00<00:01, 60820.20 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  28%|##8       | 34167/120000 [00:00<00:02, 40616.42 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  33%|###2      | 39103/120000 [00:00<00:01, 40479.56 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  37%|###7      | 44674/120000 [00:01<00:01, 44176.21 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  43%|####2     | 51144/120000 [00:01<00:01, 49447.86 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  49%|####8     | 58491/120000 [00:01<00:01, 55842.10 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  55%|#####4    | 65725/120000 [00:01<00:00, 60305.70 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  61%|######    | 73009/120000 [00:01<00:00, 63741.64 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  67%|######6   | 80044/120000 [00:01<00:00, 65503.37 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  73%|#######2  | 87355/120000 [00:01<00:00, 67533.32 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  79%|#######8  | 94672/120000 [00:01<00:00, 68979.46 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  85%|########5 | 102327/120000 [00:01<00:00, 71135.34 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  91%|#########1| 109514/120000 [00:01<00:00, 71239.01 examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-train.tfrecord*...:  97%|#########7| 116864/120000 [00:02<00:00, 71902.20 examples/s]\u001b[A\n",
      "\n",
      "                                                                                                                                                                                       \u001b[A\n",
      "                                                                \n",
      "\n",
      "Generating splits...:   0%|          | 0/2 [00:13<?, ? splits/s]\n",
      "Generating splits...:  50%|#####     | 1/2 [00:13<00:13, 13.23s/ splits]\n",
      "\n",
      "Generating test examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
      "\n",
      "Generating test examples...: 1492 examples [00:00, 14868.44 examples/s]\u001b[A\n",
      "\n",
      "Generating test examples...: 3013 examples [00:00, 15061.36 examples/s]\u001b[A\n",
      "\n",
      "Generating test examples...: 4520 examples [00:00, 14652.25 examples/s]\u001b[A\n",
      "\n",
      "Generating test examples...: 6007 examples [00:00, 14713.49 examples/s]\u001b[A\n",
      "\n",
      "Generating test examples...: 7480 examples [00:00, 14577.55 examples/s]\u001b[A\n",
      "\n",
      "                                                                       \u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-test.tfrecord*...:   0%|          | 0/7600 [00:00<?, ? examples/s]\u001b[A\n",
      "\n",
      "Shuffling C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0.incomplete22D0OG\\ag_news_subset-test.tfrecord*...:  65%|######4   | 4938/7600 [00:00<00:00, 49141.74 examples/s]\u001b[A\n",
      "\n",
      "                                                                                                                                                                                  \u001b[A\n",
      "                                                                        \n",
      "\n",
      "Generating splits...:  50%|#####     | 1/2 [00:13<00:13, 13.23s/ splits]\n",
      "Generating splits...: 100%|##########| 2/2 [00:13<00:00,  5.85s/ splits]\n",
      "                                                                        \n"
     ]
    }
   ],
   "source": [
    "#register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
    "!python -m tensorflow_datasets.scripts.download_and_prepare \\\n",
    "        --register_checksums --datasets=ag_news_subset\n",
    "\n",
    "# https://www.tensorflow.org/datasets/splits\n",
    "# The full `train` and `test` splits, interleaved together.\n",
    "ri = tfds.core.ReadInstruction('train') + tfds.core.ReadInstruction('test')\n",
    "dataset_all, info = tfds.load('ag_news_subset', with_info=True,  split=ri, as_supervised=True)\n",
    "text_only_dataset_all=dataset_all.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3fb713",
   "metadata": {},
   "source": [
    "Let's conduct exploratory data analysis of this ag_news_subset dataset. We combined the training and test data for a total of 127,600 news articles. We can begin by observing the first 10 rows of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0b71ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_14a30\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_14a30_level0_col0\" class=\"col_heading level0 col0\" >description</th>\n",
       "      <th id=\"T_14a30_level0_col1\" class=\"col_heading level0 col1\" >label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_14a30_row0_col0\" class=\"data row0 col0\" >AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.</td>\n",
       "      <td id=\"T_14a30_row0_col1\" class=\"data row0 col1\" >3 (Sci/Tech)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_14a30_row1_col0\" class=\"data row1 col0\" >Reuters - Major League Baseball\\Monday announced a decision on the appeal filed by Chicago Cubs\\pitcher Kerry Wood regarding a suspension stemming from an\\incident earlier this season.</td>\n",
       "      <td id=\"T_14a30_row1_col1\" class=\"data row1 col1\" >1 (Sports)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_14a30_row2_col0\" class=\"data row2 col0\" >President Bush #39;s quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.</td>\n",
       "      <td id=\"T_14a30_row2_col1\" class=\"data row2 col1\" >2 (Business)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_14a30_row3_col0\" class=\"data row3 col0\" >Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.</td>\n",
       "      <td id=\"T_14a30_row3_col1\" class=\"data row3 col1\" >3 (Sci/Tech)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_14a30_row4_col0\" class=\"data row4 col0\" >London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.</td>\n",
       "      <td id=\"T_14a30_row4_col1\" class=\"data row4 col1\" >1 (Sports)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_14a30_row5_col0\" class=\"data row5 col0\" >TOKYO - Sony Corp. is banking on the \\$3 billion deal to acquire Hollywood studio Metro-Goldwyn-Mayer Inc...</td>\n",
       "      <td id=\"T_14a30_row5_col1\" class=\"data row5 col1\" >0 (World)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_14a30_row6_col0\" class=\"data row6 col0\" >Giant pandas may well prefer bamboo to laptops, but wireless technology is helping researchers in China in their efforts to protect the engandered animals living in the remote Wolong Nature Reserve.</td>\n",
       "      <td id=\"T_14a30_row6_col1\" class=\"data row6 col1\" >3 (Sci/Tech)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_14a30_row7_col0\" class=\"data row7 col0\" >VILNIUS, Lithuania - Lithuania #39;s main parties formed an alliance to try to keep a Russian-born tycoon and his populist promises out of the government in Sunday #39;s second round of parliamentary elections in this Baltic country.</td>\n",
       "      <td id=\"T_14a30_row7_col1\" class=\"data row7 col1\" >0 (World)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_14a30_row8_col0\" class=\"data row8 col0\" >Witnesses in the trial of a US soldier charged with abusing prisoners at Abu Ghraib have told the court that the CIA sometimes directed abuse and orders were received from military command to toughen interrogations.</td>\n",
       "      <td id=\"T_14a30_row8_col1\" class=\"data row8 col1\" >0 (World)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_14a30_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_14a30_row9_col0\" class=\"data row9 col0\" >Dan Olsen of Ponte Vedra Beach, Fla., shot a 7-under 65 Thursday to take a one-shot lead after two rounds of the PGA Tour qualifying tournament.</td>\n",
       "      <td id=\"T_14a30_row9_col1\" class=\"data row9 col1\" >1 (Sports)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "                                         description  label\n",
       "0  b'AMD #39;s new dual-core Opteron chip is desi...      3\n",
       "1  b'Reuters - Major League Baseball\\\\Monday anno...      1\n",
       "2  b'President Bush #39;s  quot;revenue-neutral q...      2\n",
       "3  b'Britain will run out of leading scientists u...      3\n",
       "4  b'London, England (Sports Network) - England m...      1\n",
       "5  b'TOKYO - Sony Corp. is banking on the \\\\$3 bi...      0\n",
       "6  b'Giant pandas may well prefer bamboo to lapto...      3\n",
       "7  b'VILNIUS, Lithuania - Lithuania #39;s main pa...      0\n",
       "8  b'Witnesses in the trial of a US soldier charg...      0\n",
       "9  b'Dan Olsen of Ponte Vedra Beach, Fla., shot a...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.as_dataframe(dataset_all.take(10),info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c00ff",
   "metadata": {},
   "source": [
    "Let's review the labels for the categories of articles in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2000da97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary:  {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n"
     ]
    }
   ],
   "source": [
    "categories =dict(enumerate(info.features[\"label\"].names))\n",
    "print(f'Dictionary: ',categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed216c99",
   "metadata": {},
   "source": [
    "Let's observe the number of observations that correspond to each class in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d09e842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sci/Tech', 31900), ('Sports', 31900), ('Business', 31900), ('World', 31900)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_categories = [categories[label] for label in dataset_all.map(lambda text, label: label).as_numpy_iterator()]\n",
    "Counter(train_categories).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8952a62",
   "metadata": {},
   "source": [
    "We see that the 127,600 articles are evenly distributed across the four classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d2e7c",
   "metadata": {},
   "source": [
    "Let's do a bit of data preprocessing to enable further exploratory data analysis. \n",
    "\n",
    "We can start by making the corpus all lowercase, stripping punctuation, and removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "517541fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stopwords(input_text):\n",
    "    lowercase = tf.strings.lower(input_text)\n",
    "    stripped_punct = tf.strings.regex_replace(lowercase\n",
    "                                  ,'[%s]' % re.escape(string.punctuation)\n",
    "                                  ,'')\n",
    "    return tf.strings.regex_replace(stripped_punct, r'\\b(' + r'|'.join(STOPWORDS) + r')\\b\\s*',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae1a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords',quiet=True)\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "244cbe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.8 s\n",
      "Wall time: 2min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_tokens = None\n",
    "text_vectorization=layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_stopwords\n",
    ")\n",
    "text_vectorization.adapt(text_only_dataset_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6604c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 15s\n",
      "Wall time: 10min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_sizes = []\n",
    "corpus = []\n",
    "for example, _ in dataset_all.as_numpy_iterator():\n",
    "  enc_example = text_vectorization(example)\n",
    "  doc_sizes.append(len(enc_example))\n",
    "  corpus+=list(enc_example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82b5d477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2579419 words in the corpus of 127600 news articles.\n",
      "Each news article has between 2 and 95 tokens in it.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(corpus)} words in the corpus of {len(doc_sizes)} news articles.\")\n",
    "print(f\"Each news article has between {min(doc_sizes)} and {max(doc_sizes)} tokens in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "706a8ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 95827 vocabulary words in the corpus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(text_vectorization.get_vocabulary())} vocabulary words in the corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4ccb3d",
   "metadata": {},
   "source": [
    "Let's observe the first 50 words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ef45f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' '[UNK]' '39s' 'said' 'new' 'us' 'reuters' 'ap' 'two' 'first' 'monday'\n",
      " 'wednesday' 'tuesday' 'thursday' 'company' 'friday' 'inc' 'one' 'world'\n",
      " 'yesterday' 'last' 'york' 'year' 'president' 'million' 'oil' 'corp'\n",
      " 'united' 'would' 'sunday' 'years' 'week' 'people' 'today' 'three'\n",
      " 'government' 'could' 'quot' 'group' 'time' 'percent' 'game' 'saturday'\n",
      " 'software' 'night' 'next' 'prices' 'iraq' 'security' 'announced']\n"
     ]
    }
   ],
   "source": [
    "vocab = np.array(text_vectorization.get_vocabulary())\n",
    "print(vocab[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258b1209",
   "metadata": {},
   "source": [
    "Let's examine the distribution of the number of tokens per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da883b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.hist(doc_sizes, bins=20,range = (0,80))\n",
    "plt.xlabel(\"Tokens Per Document\")\n",
    "plt.ylabel(\"Number of AG News Articles\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db133d",
   "metadata": {},
   "source": [
    "### 1.3) Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd32b8e",
   "metadata": {},
   "source": [
    "Now that we've conducted exploratory data analysis, let's preprocess the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8500ae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO[build.py]: Loading dataset ag_news_subset from imports: tensorflow_datasets.datasets.ag_news_subset.ag_news_subset_dataset_builder\n",
      "INFO[dataset_info.py]: Load dataset info from C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0\n",
      "INFO[build.py]: download_and_prepare for dataset ag_news_subset/1.0.0...\n",
      "INFO[dataset_builder.py]: Reusing dataset ag_news_subset (C:\\Users\\steve\\tensorflow_datasets\\ag_news_subset\\1.0.0)\n",
      "INFO[build.py]: Dataset generation complete...\n",
      "\n",
      "tfds.core.DatasetInfo(\n",
      "    name='ag_news_subset',\n",
      "    full_name='ag_news_subset/1.0.0',\n",
      "    description=\"\"\"\n",
      "    AG is a collection of more than 1 million news articles. News articles have been\n",
      "    gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of\n",
      "    activity. ComeToMyHead is an academic news search engine which has been running\n",
      "    since July, 2004. The dataset is provided by the academic comunity for research\n",
      "    purposes in data mining (clustering, classification, etc), information retrieval\n",
      "    (ranking, search, etc), xml, data compression, data streaming, and any other\n",
      "    non-commercial activity. For more information, please refer to the link\n",
      "    http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
      "    \n",
      "    The AG's news topic classification dataset is constructed by Xiang Zhang\n",
      "    (xiang.zhang@nyu.edu) from the dataset above. It is used as a text\n",
      "    classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann\n",
      "    LeCun. Character-level Convolutional Networks for Text Classification. Advances\n",
      "    in Neural Information Processing Systems 28 (NIPS 2015).\n",
      "    \n",
      "    The AG's news topic classification dataset is constructed by choosing 4 largest\n",
      "    classes from the original corpus. Each class contains 30,000 training samples\n",
      "    and 1,900 testing samples. The total number of training samples is 120,000 and\n",
      "    testing 7,600.\n",
      "    \"\"\",\n",
      "    homepage='https://arxiv.org/abs/1509.01626',\n",
      "    data_path='C:\\\\Users\\\\steve\\\\tensorflow_datasets\\\\ag_news_subset\\\\1.0.0',\n",
      "    file_format=tfrecord,\n",
      "    download_size=11.24 MiB,\n",
      "    dataset_size=35.79 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'description': Text(shape=(), dtype=string),\n",
      "        'label': ClassLabel(shape=(), dtype=int64, num_classes=4),\n",
      "        'title': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "    supervised_keys=('description', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=7600, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=120000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@misc{zhang2015characterlevel,\n",
      "        title={Character-level Convolutional Networks for Text Classification},\n",
      "        author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n",
      "        year={2015},\n",
      "        eprint={1509.01626},\n",
      "        archivePrefix={arXiv},\n",
      "        primaryClass={cs.LG}\n",
      "    }\"\"\",\n",
      ")\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1105 01:37:58.398307 30340 download_and_prepare.py:46] ***`tfds build` should be used instead of `download_and_prepare`.***\n"
     ]
    }
   ],
   "source": [
    "# register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
    "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=ag_news_subset\n",
    "\n",
    "dataset,info=\\\n",
    "tfds.load('ag_news_subset', with_info=True,  split=['train[:95%]','train[95%:]', 'test'],batch_size = 32\n",
    "          , as_supervised=True)\n",
    "\n",
    "train_ds, val_ds, test_ds = dataset\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1805b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 96\n",
    "max_tokens = 1000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    "    standardize=custom_stopwords\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fa586",
   "metadata": {},
   "source": [
    "## 2) Model 1 - Bi-Directional Long Short Term Memory (LSTM) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153925b7",
   "metadata": {},
   "source": [
    "### 2.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30b54a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         256000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               73984     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 330,244\n",
      "Trainable params: 330,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "3563/3563 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.8256"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model_One\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model_One\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563/3563 [==============================] - 281s 76ms/step - loss: 0.4885 - accuracy: 0.8256 - val_loss: 0.4036 - val_accuracy: 0.8570\n",
      "Epoch 2/200\n",
      "3562/3563 [============================>.] - ETA: 0s - loss: 0.4150 - accuracy: 0.8528"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model_One\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model_One\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563/3563 [==============================] - 278s 78ms/step - loss: 0.4150 - accuracy: 0.8528 - val_loss: 0.3908 - val_accuracy: 0.8628\n",
      "Epoch 3/200\n",
      "3563/3563 [==============================] - ETA: 0s - loss: 0.3988 - accuracy: 0.8584"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model_One\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Model_One\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563/3563 [==============================] - 388s 109ms/step - loss: 0.3988 - accuracy: 0.8584 - val_loss: 0.3858 - val_accuracy: 0.8633\n",
      "Epoch 4/200\n",
      " 716/3563 [=====>........................] - ETA: 12:49 - loss: 0.3830 - accuracy: 0.8633"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel_One\u001b[39m\u001b[38;5;124m\"\u001b[39m,save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m     ,tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     20\u001b[0m ]\n\u001b[0;32m     24\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m---> 26\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint_train_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint_val_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     29\u001b[0m runtime \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k.clear_session()\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens\n",
    "                            ,output_dim=256\n",
    "                            ,mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"Model_One\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "history=model.fit(int_train_ds, validation_data=int_val_ds, epochs=200, callbacks=callbacks)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "runtime = end_time - start_time\n",
    "print(f\"The runtime to fit this model was: {runtime}.\")\n",
    "\n",
    "\n",
    "model = keras.models.load_model(\"Model_One\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5263e5",
   "metadata": {},
   "source": [
    "### 2.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b89eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8591eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef637b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff79524",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1d069",
   "metadata": {},
   "source": [
    "## 3) Model 2 - 1-Dimensional Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39811306",
   "metadata": {},
   "source": [
    "### 3.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402df02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Conv1D(filters=32, kernel_size=3, activation='relu')(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"Model_Two\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "history=model.fit(int_train_ds, validation_data=int_val_ds, epochs=200, callbacks=callbacks)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "runtime = end_time - start_time\n",
    "print(f\"The runtime to fit this model was: {runtime}.\")\n",
    "\n",
    "model = keras.models.load_model(\"Model_Two\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef132164",
   "metadata": {},
   "source": [
    "### 3.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c732014",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac48e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb03686",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70547730",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba60a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb83f20",
   "metadata": {},
   "source": [
    "## 4) Model 3 - Dense Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef5f6fd",
   "metadata": {},
   "source": [
    "### 4.1) Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b37c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99862b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd61304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2eec383",
   "metadata": {},
   "source": [
    "### 4.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625965a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8360da",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e1579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba435f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317e2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26aa64",
   "metadata": {},
   "source": [
    "## 5) Model 4 - Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f651b311",
   "metadata": {},
   "source": [
    "### 5.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6dd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112a6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91ce7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d04754c5",
   "metadata": {},
   "source": [
    "### 5.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac98d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5911cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f3a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba731df7",
   "metadata": {},
   "source": [
    "## 6) Model 5 - ____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff6b080",
   "metadata": {},
   "source": [
    "### 6.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee6a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a20fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0f4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74615d65",
   "metadata": {},
   "source": [
    "### 6.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140878e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e0900",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38bdf8",
   "metadata": {},
   "source": [
    "## 7) Model 6 - _____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919ac03",
   "metadata": {},
   "source": [
    "### 7.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d86890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d45f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05555cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d610a96",
   "metadata": {},
   "source": [
    "### 7.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30c354",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61979048",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7035a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63569340",
   "metadata": {},
   "source": [
    "## 8) Model 7 - _________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c047cb",
   "metadata": {},
   "source": [
    "### 8.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87639fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787934fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac5f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28a15e0b",
   "metadata": {},
   "source": [
    "### 8.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d85ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38969ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1fd443",
   "metadata": {},
   "source": [
    "## 9) Model 8 - ___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480ca2f0",
   "metadata": {},
   "source": [
    "### 9.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634fdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91030918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa055a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1632ee",
   "metadata": {},
   "source": [
    "### 9.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be8acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba009162",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffd8ff",
   "metadata": {},
   "source": [
    "## 10) Model 9 - ___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2b740",
   "metadata": {},
   "source": [
    "### 10.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf6864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc9d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f690523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e51a6d8b",
   "metadata": {},
   "source": [
    "### 10.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d76530",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10158f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2a2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4261a6df",
   "metadata": {},
   "source": [
    "## 11) Model 10 - _______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f529ec",
   "metadata": {},
   "source": [
    "### 11.1) Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ddf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bba5d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b226f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f596a50b",
   "metadata": {},
   "source": [
    "### 11.2) Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)\n",
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(16,12))\n",
    "plt.tight_layout()\n",
    "display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\n",
    "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6da99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_report(y_test, pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8589647",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test,pred_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
